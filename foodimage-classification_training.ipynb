{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Custom Dataset Class (Normal):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoodDataset(Dataset):\n",
    "    def __init__(self, file_paths, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(set([os.path.dirname(fp).split('/')[-1] for fp in file_paths]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        image = Image.open(file_path).convert(\"RGB\")\n",
    "        #image = plt.imread(file_path)\n",
    "        label = self.classes.index(os.path.dirname(file_path).split('/')[-1])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Training and Testing Splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split(file_path, dataset_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return [os.path.join(dataset_path, line.strip() + \".jpg\") for line in lines]\n",
    "\n",
    "# Paths\n",
    "dataset_path = \"food-101/images\"\n",
    "meta_path = \"food-101/meta\"\n",
    "\n",
    "train_files = load_split(os.path.join(meta_path, \"train.txt\"), dataset_path)\n",
    "test_files = load_split(os.path.join(meta_path, \"test.txt\"), dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FoodDataset(train_files, transform=train_transform)\n",
    "test_dataset = FoodDataset(test_files, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pretrained ResNet\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_classes = len(train_dataset.classes)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss and Optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, labels in tqdm(dataloader, desc=\"Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return total_loss / len(dataloader), accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Script (with Chckpoints):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found. Starting from scratch.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2368/2368 [2:18:39<00:00,  3.51s/it]  \n",
      "Evaluating: 100%|██████████| 790/790 [15:17<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.6589\n",
      "Validation Loss: 2.0902, Accuracy: 0.4764\n",
      "Checkpoint saved at checkpoints/best_model.pth\n",
      "Checkpoint saved at checkpoints/checkpoint_epoch_1.pth\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2368/2368 [2:02:00<00:00,  3.09s/it]  \n",
      "Evaluating: 100%|██████████| 790/790 [15:56<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.8540\n",
      "Validation Loss: 1.5668, Accuracy: 0.5811\n",
      "Checkpoint saved at checkpoints/best_model.pth\n",
      "Checkpoint saved at checkpoints/checkpoint_epoch_2.pth\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2368/2368 [2:05:11<00:00,  3.17s/it]  \n",
      "Evaluating: 100%|██████████| 790/790 [15:52<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5223\n",
      "Validation Loss: 1.4937, Accuracy: 0.6128\n",
      "Checkpoint saved at checkpoints/best_model.pth\n",
      "Checkpoint saved at checkpoints/checkpoint_epoch_3.pth\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2368/2368 [1:52:19<00:00,  2.85s/it]\n",
      "Evaluating: 100%|██████████| 790/790 [13:07<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2923\n",
      "Validation Loss: 1.2698, Accuracy: 0.6604\n",
      "Checkpoint saved at checkpoints/best_model.pth\n",
      "Checkpoint saved at checkpoints/checkpoint_epoch_4.pth\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2368/2368 [1:37:02<00:00,  2.46s/it]\n",
      "Evaluating: 100%|██████████| 790/790 [12:53<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0916\n",
      "Validation Loss: 1.3141, Accuracy: 0.6565\n",
      "Checkpoint saved at checkpoints/checkpoint_epoch_5.pth\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2368/2368 [1:36:22<00:00,  2.44s/it]\n",
      "Evaluating: 100%|██████████| 790/790 [13:01<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9286\n",
      "Validation Loss: 1.2713, Accuracy: 0.6710\n",
      "Checkpoint saved at checkpoints/best_model.pth\n",
      "Checkpoint saved at checkpoints/checkpoint_epoch_6.pth\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2368/2368 [1:35:41<00:00,  2.42s/it]\n",
      "Evaluating: 100%|██████████| 790/790 [12:39<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7856\n",
      "Validation Loss: 1.2670, Accuracy: 0.6831\n",
      "Checkpoint saved at checkpoints/best_model.pth\n",
      "Checkpoint saved at checkpoints/checkpoint_epoch_7.pth\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2368/2368 [1:31:53<00:00,  2.33s/it]\n",
      "Evaluating: 100%|██████████| 790/790 [11:22<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6590\n",
      "Validation Loss: 1.3343, Accuracy: 0.6751\n",
      "Checkpoint saved at checkpoints/checkpoint_epoch_8.pth\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2368/2368 [1:22:07<00:00,  2.08s/it]\n",
      "Evaluating: 100%|██████████| 790/790 [11:17<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5619\n",
      "Validation Loss: 1.3852, Accuracy: 0.6724\n",
      "Checkpoint saved at checkpoints/checkpoint_epoch_9.pth\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2368/2368 [1:22:44<00:00,  2.10s/it]\n",
      "Evaluating: 100%|██████████| 790/790 [11:25<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4756\n",
      "Validation Loss: 1.4353, Accuracy: 0.6794\n",
      "Checkpoint saved at checkpoints/checkpoint_epoch_10.pth\n",
      "Training Complete.\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = 'checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, loss, accuracy, filename=\"checkpoint.pth\"):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, filename)\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "def load_checkpoint(filename=\"checkpoint.pth\"):\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, filename)\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        print(f\"Checkpoint loaded from {checkpoint_path}\")\n",
    "        return checkpoint\n",
    "    else:\n",
    "        print(\"No checkpoint found. Starting from scratch.\")\n",
    "        return None\n",
    "\n",
    "# Modify the training loop to integrate checkpointing\n",
    "num_epochs = 10\n",
    "best_accuracy = 0.0\n",
    "\n",
    "# Check if there is a saved checkpoint to resume from\n",
    "checkpoint = load_checkpoint()\n",
    "\n",
    "if checkpoint:\n",
    "    # Load the model and optimizer states from checkpoint\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1  # Resume from the next epoch\n",
    "    best_accuracy = checkpoint['accuracy']  # Retain best accuracy\n",
    "else:\n",
    "    start_epoch = 0\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_accuracy = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Save the best model\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        save_checkpoint(epoch, model, optimizer, val_loss, val_accuracy, filename=\"best_model.pth\")\n",
    "\n",
    "    # Save a checkpoint periodically (e.g., after every epoch)\n",
    "    save_checkpoint(epoch, model, optimizer, val_loss, val_accuracy, filename=f\"checkpoint_epoch_{epoch+1}.pth\")\n",
    "\n",
    "print(\"Training Complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meal-analyzer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
